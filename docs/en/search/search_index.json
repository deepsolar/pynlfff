{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Material for MkDocs"},{"location":"changelog/","text":"Archived March 2022 for first release.","title":"Changelog"},{"location":"construction/","text":"The whole database construction is divided into data filtering, raw data downloading, boundary condition preparation, batch calculation, tagging, data archiving and publishing, you can find the construction details from this article .","title":"Construction"},{"location":"contact/","text":"For small amounts of data, you can search and download directly from Online Search and Download , and you can download summary information from Dataset Summary Information to download the summary information. For data over 100GB (100GB-200TB), we recommend contacting us first via the email below, and then discussing transfer by mailing the hard disk, etc. Long Xu ( xszhu@bao.ac.cn ) Xin Huang ( huangxin@nssc.ac.cn )","title":"Contact"},{"location":"download/","text":"Overall \u00b6 Summary Information Search Online Raw File Data \u00b6 We use drms to download the sharp cea file (Bp.fits, Bt.fits, Br.fits) from JSOC as raw file and save it in the raw-202203 folder, which can be found in this article with detailed filtering criteria. NLFFF File Data \u00b6 The product files are stored in the archive-202203 folder, the folder naming convention can be found in this article or read in this sqlite database, each folder has the contents listed in the table below You can also filter and query here . For small amounts of data (<100GB) you can try downloading directly through the web, for large amounts of data (100GB-200TB) we recommend contacting us first to deliver it by sending a hard drive, etc. Generation stage File name Description prepare grid1.ini,grid2.ini,grid3.ini Information about the grid used for the corresponding level mask1.dat,mask2.dat,mask3.dat The mask data used for the corresponding level allboundaries1.dat, allboundaries2.dat, allboundaries3.dat The boundaries data used for the corresponding level boundary.ini Boundary and algorithm information process Bout.bin Nonlinear force-free field B0.bin Potential field, due to storage problems, this part of the data is partially saved NLFFFquality1.log, NLFFFquality2.log, NLFFFquality3.log Corresponding grade of product quality prot1.log, prot2.log, prot3.log Corresponding level iteration log information step1.log, step2.log, step3.log Information on the number of iterative steps for the corresponding level Energy.log Run Energy Log, If you only run to grid1, this part may not have archive run.log Run Print Detail Log Label File Data \u00b6 This is the data generated when generating the label, which you can access through the links in the table below. Generation stage File name Description raw_file flare_raw The folder containing the original flare information for download raw_file all_harps_with_noaa_ars.txt Mapping update for HARP number and NOAA number prepare knoaa_vflaretimelist.pickle The dictionary with key NOAA number,value flare time list is saved as python pickle prepare ksharp_vnoaa.pickle Key is HARP number,value is NOAA number list of dictionaries saved as python pickle process label.csv Sample Label Information NLFFF and flare info in sqlite database \u00b6 You can download the sqlite database file with nlfff and flare information from sqlite-202203/a202203-nlfff.db . For NLFFF data is described in the following table: Col Name Type Description harpnum_trec timestamp HARP number and time from raw fits name sync with raw fits sync raw find from http://jsoc.stanford.edu/ajax/lookdata.html?ds=hmi.sharp_cea_720s grid_x int4 we finally can calculate the nx,ny,nz corresponding to the level, that is, the nx,ny,nz corresponding to the saved Bout.bin file grid_y int4 grid_z int4 bout_maxlevel int4 The final calculated level,The level of the last saved bout bout_quality_value float8 Set to True if quality is less than 30, else False. bout_quality bool The final calculated quality,The quality of the last saved bout bout_path text The path where Bout is saved, and other files in the same subdirectory as Bout bout_md5 varchar(32) The md5 of Bout bout_size int8 The size of Bout batch int8 Calculated batches, other calculated batches may be available in the future now_flare_level int4 Current Flare Level now_flare_id int4 Current Flare id h6_flare_level, h12_flare_level, h24_flare_level, h48_flare_level int4 Maximum flare levels in 6, 12, 24 and 48 hours, respectively h6_flare_id, h12_flare_id, h24_flare_id, h48_flare_id int4 The ids corresponding to the maximum flare levels in 6,12,24,48 hours, respectively h24_posmx int8 0 - No MX or CMX level flares in hour24 or 48;1 - MX or CMX level flares within hour24 or 48;2 - MX or CMX Flares occur within hour24 or 48 and the sample does not have a MX or CMX level flare at that moment h24_poscmx int8 h48_posmx int8 h48_poscmx int8 h24_delta05 int8 Maximum change in grade in 24 hours h48_delta05 int8 Maximum change in grade in 48 hours For flare data is described in the following table: Col Name Type Description deeps_flare_id int4 The id that uniquely identifies the flare information in the deepsolar database system start_datetime timestamp Flare start time peak_datetime timestamp Flare end time end_datetime timestamp Flare peaking time xray_class varchar(1) Flare level class xray_intensity int4 Intensity of raw data multiplied by 10 latitude int4 latitude longtitude int4 longtitude noaa_ar int4 Corresponding NOAA active region number source varchar(16) Data source","title":"Download"},{"location":"download/#overall","text":"Summary Information Search Online","title":"Overall"},{"location":"download/#raw-file-data","text":"We use drms to download the sharp cea file (Bp.fits, Bt.fits, Br.fits) from JSOC as raw file and save it in the raw-202203 folder, which can be found in this article with detailed filtering criteria.","title":"Raw File Data"},{"location":"download/#nlfff-file-data","text":"The product files are stored in the archive-202203 folder, the folder naming convention can be found in this article or read in this sqlite database, each folder has the contents listed in the table below You can also filter and query here . For small amounts of data (<100GB) you can try downloading directly through the web, for large amounts of data (100GB-200TB) we recommend contacting us first to deliver it by sending a hard drive, etc. Generation stage File name Description prepare grid1.ini,grid2.ini,grid3.ini Information about the grid used for the corresponding level mask1.dat,mask2.dat,mask3.dat The mask data used for the corresponding level allboundaries1.dat, allboundaries2.dat, allboundaries3.dat The boundaries data used for the corresponding level boundary.ini Boundary and algorithm information process Bout.bin Nonlinear force-free field B0.bin Potential field, due to storage problems, this part of the data is partially saved NLFFFquality1.log, NLFFFquality2.log, NLFFFquality3.log Corresponding grade of product quality prot1.log, prot2.log, prot3.log Corresponding level iteration log information step1.log, step2.log, step3.log Information on the number of iterative steps for the corresponding level Energy.log Run Energy Log, If you only run to grid1, this part may not have archive run.log Run Print Detail Log","title":"NLFFF File Data"},{"location":"download/#label-file-data","text":"This is the data generated when generating the label, which you can access through the links in the table below. Generation stage File name Description raw_file flare_raw The folder containing the original flare information for download raw_file all_harps_with_noaa_ars.txt Mapping update for HARP number and NOAA number prepare knoaa_vflaretimelist.pickle The dictionary with key NOAA number,value flare time list is saved as python pickle prepare ksharp_vnoaa.pickle Key is HARP number,value is NOAA number list of dictionaries saved as python pickle process label.csv Sample Label Information","title":"Label File Data"},{"location":"download/#nlfff-and-flare-info-in-sqlite-database","text":"You can download the sqlite database file with nlfff and flare information from sqlite-202203/a202203-nlfff.db . For NLFFF data is described in the following table: Col Name Type Description harpnum_trec timestamp HARP number and time from raw fits name sync with raw fits sync raw find from http://jsoc.stanford.edu/ajax/lookdata.html?ds=hmi.sharp_cea_720s grid_x int4 we finally can calculate the nx,ny,nz corresponding to the level, that is, the nx,ny,nz corresponding to the saved Bout.bin file grid_y int4 grid_z int4 bout_maxlevel int4 The final calculated level,The level of the last saved bout bout_quality_value float8 Set to True if quality is less than 30, else False. bout_quality bool The final calculated quality,The quality of the last saved bout bout_path text The path where Bout is saved, and other files in the same subdirectory as Bout bout_md5 varchar(32) The md5 of Bout bout_size int8 The size of Bout batch int8 Calculated batches, other calculated batches may be available in the future now_flare_level int4 Current Flare Level now_flare_id int4 Current Flare id h6_flare_level, h12_flare_level, h24_flare_level, h48_flare_level int4 Maximum flare levels in 6, 12, 24 and 48 hours, respectively h6_flare_id, h12_flare_id, h24_flare_id, h48_flare_id int4 The ids corresponding to the maximum flare levels in 6,12,24,48 hours, respectively h24_posmx int8 0 - No MX or CMX level flares in hour24 or 48;1 - MX or CMX level flares within hour24 or 48;2 - MX or CMX Flares occur within hour24 or 48 and the sample does not have a MX or CMX level flare at that moment h24_poscmx int8 h48_posmx int8 h48_poscmx int8 h24_delta05 int8 Maximum change in grade in 24 hours h48_delta05 int8 Maximum change in grade in 48 hours For flare data is described in the following table: Col Name Type Description deeps_flare_id int4 The id that uniquely identifies the flare information in the deepsolar database system start_datetime timestamp Flare start time peak_datetime timestamp Flare end time end_datetime timestamp Flare peaking time xray_class varchar(1) Flare level class xray_intensity int4 Intensity of raw data multiplied by 10 latitude int4 latitude longtitude int4 longtitude noaa_ar int4 Corresponding NOAA active region number source varchar(16) Data source","title":"NLFFF and flare info in sqlite database"},{"location":"home/","text":"Based on the vector magnetic field data observed by Helioseismic and Magnetic Imager( HMI ) on the Solar Dynamics Observatory (SDO), we have built a 3D solar active region magnetic field dataset using the Nonlinear Force-Free Coronal Magnetic Field ( NLFFF )Extrapolations. The current total is over 200 TB, 73,000 3D magnetograms spanning the years 2010 to 2019, with a temporal resolution of 96 minutes and a spatial resolution consistent with the original sharp cea magnetograms .","title":"Home"},{"location":"product/","text":"The main product file is Bout.bin file, which stores nlfff magnetic field data, the storage format is a four-dimensional array according to line priority (c style), the 0 th dimension of the array is the component flag, 123 dimensions indicate the xyz coordinate values of Bx, By, Bz respectively, xyz take the value range with the last layer of grid, the storage data unit is Gaussian, you can get more from this article More details, If you use python, you can also use pynlfff to read and write operations, if you use other languages, you can refer to python implementation.","title":"Product"},{"location":"reference/","text":"This Dataset and Articles \u00b6 TODO Algorithms used \u00b6 We use Wiegelmann's Nonlinear Force-Free Coronal Magnetic Field ( NLFFF ) Extrapolations algorithm for extrapolation. @article { Wiegelmann2012Nov, author = { Wiegelmann, T. and Thalmann, J. K. and Inhester, B. and Tadesse, T. and Sun, X. and Hoeksema, J. T. } , title = {{ How Should One Optimize Nonlinear Force-Free Coronal Magnetic Field Extrapolations from SDO/HMI Vector Magnetograms? }} , journal = { Sol. Phys. } , volume = { 281 } , number = { 1 } , pages = { 37--51 } , year = { 2012 } , month = nov, issn = { 1573-093X } , publisher = { Springer Netherlands } , doi = { 10.1007/s11207-012-9966-z } }","title":"Reference"},{"location":"reference/#this-dataset-and-articles","text":"TODO","title":"This Dataset and Articles"},{"location":"reference/#algorithms-used","text":"We use Wiegelmann's Nonlinear Force-Free Coronal Magnetic Field ( NLFFF ) Extrapolations algorithm for extrapolation. @article { Wiegelmann2012Nov, author = { Wiegelmann, T. and Thalmann, J. K. and Inhester, B. and Tadesse, T. and Sun, X. and Hoeksema, J. T. } , title = {{ How Should One Optimize Nonlinear Force-Free Coronal Magnetic Field Extrapolations from SDO/HMI Vector Magnetograms? }} , journal = { Sol. Phys. } , volume = { 281 } , number = { 1 } , pages = { 37--51 } , year = { 2012 } , month = nov, issn = { 1573-093X } , publisher = { Springer Netherlands } , doi = { 10.1007/s11207-012-9966-z } }","title":"Algorithms used"},{"location":"toolkit/","text":"Toolkit Use \u00b6 Nlfff file reading and writing \u00b6 Create Object \u00b6 from pynlfff.pyproduct import file r = file . NlfffFile () Read data \u00b6 Read size from grid \u00b6 r = file . NlfffFile () grid_path = r \"\\product0\\grid3.ini\" s = r . get_size_from_grid ( grid_path ) print ( s ) [96, 228, 124] Read Bout \u00b6 Method 1: Given Bout.bin and the corresponding grid.ini file r = file . NlfffFile () bout_bin_path = r \"\\product0\\Bout.bin\" grid_path = r \"\\product0\\grid3.ini\" s = r . read_bin ( bout_bin_path , grid_path = grid_path ) print ( s . shape ) print ( s ) (3, 96, 228, 124) [[[[ -4.85755745 -3.69001267 -4.07361282 ... -0.26802718 -0.26802718 -0.26802718] [ -2.37227954 -3.32715352 -4.60636354 ... -0.26799428 -0.26799428 -0.26799428] [ 3.09420419 -2.96429436 -5.13911427 ... -0.26796139 -0.26796139 -0.26796139] ... ... [ 7.94384187 6.06213112 4.73552989 ... -0.45711367 -0.45711367 -0.45711367] [ 2.05455194 4.55914711 4.73552989 ... -0.45711367 -0.45711367 -0.45711367] [ 7.05974157 4.55914711 4.73552989 ... -0.45711367 -0.45711367 -0.45711367]]]] Method 2: Given Bout.bin and the corresponding nx , ny , nz values r = file . NlfffFile () bout_bin_path = r \"\\product0\\Bout.bin\" nx = 96 ny = 228 nz = 124 s = r . read_bin ( bout_bin_path , nx = nx , ny = ny , nz = nz ) print ( s . shape ) print ( s ) Write data \u00b6 Convert file format to hdf5 \u00b6 Method 1: Given Bout.bin and the corresponding grid.ini file, and the hdf file save path r = file . NlfffFile () bout_bin_path = r \"\\product0\\Bout.bin\" grid_path = r \"\\product0\\grid3.ini\" h5_path = r \"\\product0\\Bxyz.h5\" r . tran_bin2hdf5 ( bout_bin_path , h5_path , grid_path = grid_path , overwrite = True ) Method 2: Given Bout.bin and the corresponding nx , ny , nz values, and the hdf file save path r = file . NlfffFile () bout_bin_path = r \"\\product0\\Bout.bin\" h5_path = r \"\\product0\\Bxyz.h5\" nx = 96 ny = 228 nz = 124 r . tran_bin2hdf5 ( bout_bin_path , h5_path , nx = nx , ny = ny , nz = nz , overwrite = True ) Write array data \u00b6 r = file . NlfffFile () array_data = np . random . uniform ( - 5 , 5 , size = ( 3 , 100 , 50 , 40 )) h5_path = r \"\\product0\\Bxyz.h5\" r . write_hdf5 ( array_data , h5_path , overwrite = True )","title":"Toolkit"},{"location":"toolkit/#toolkit-use","text":"","title":"Toolkit Use"},{"location":"toolkit/#nlfff-file-reading-and-writing","text":"","title":"Nlfff file reading and writing"},{"location":"toolkit/#create-object","text":"from pynlfff.pyproduct import file r = file . NlfffFile ()","title":"Create Object"},{"location":"toolkit/#read-data","text":"","title":"Read data"},{"location":"toolkit/#read-size-from-grid","text":"r = file . NlfffFile () grid_path = r \"\\product0\\grid3.ini\" s = r . get_size_from_grid ( grid_path ) print ( s ) [96, 228, 124]","title":"Read size from grid"},{"location":"toolkit/#read-bout","text":"Method 1: Given Bout.bin and the corresponding grid.ini file r = file . NlfffFile () bout_bin_path = r \"\\product0\\Bout.bin\" grid_path = r \"\\product0\\grid3.ini\" s = r . read_bin ( bout_bin_path , grid_path = grid_path ) print ( s . shape ) print ( s ) (3, 96, 228, 124) [[[[ -4.85755745 -3.69001267 -4.07361282 ... -0.26802718 -0.26802718 -0.26802718] [ -2.37227954 -3.32715352 -4.60636354 ... -0.26799428 -0.26799428 -0.26799428] [ 3.09420419 -2.96429436 -5.13911427 ... -0.26796139 -0.26796139 -0.26796139] ... ... [ 7.94384187 6.06213112 4.73552989 ... -0.45711367 -0.45711367 -0.45711367] [ 2.05455194 4.55914711 4.73552989 ... -0.45711367 -0.45711367 -0.45711367] [ 7.05974157 4.55914711 4.73552989 ... -0.45711367 -0.45711367 -0.45711367]]]] Method 2: Given Bout.bin and the corresponding nx , ny , nz values r = file . NlfffFile () bout_bin_path = r \"\\product0\\Bout.bin\" nx = 96 ny = 228 nz = 124 s = r . read_bin ( bout_bin_path , nx = nx , ny = ny , nz = nz ) print ( s . shape ) print ( s )","title":"Read Bout"},{"location":"toolkit/#write-data","text":"","title":"Write data"},{"location":"toolkit/#convert-file-format-to-hdf5","text":"Method 1: Given Bout.bin and the corresponding grid.ini file, and the hdf file save path r = file . NlfffFile () bout_bin_path = r \"\\product0\\Bout.bin\" grid_path = r \"\\product0\\grid3.ini\" h5_path = r \"\\product0\\Bxyz.h5\" r . tran_bin2hdf5 ( bout_bin_path , h5_path , grid_path = grid_path , overwrite = True ) Method 2: Given Bout.bin and the corresponding nx , ny , nz values, and the hdf file save path r = file . NlfffFile () bout_bin_path = r \"\\product0\\Bout.bin\" h5_path = r \"\\product0\\Bxyz.h5\" nx = 96 ny = 228 nz = 124 r . tran_bin2hdf5 ( bout_bin_path , h5_path , nx = nx , ny = ny , nz = nz , overwrite = True )","title":"Convert file format to hdf5"},{"location":"toolkit/#write-array-data","text":"r = file . NlfffFile () array_data = np . random . uniform ( - 5 , 5 , size = ( 3 , 100 , 50 , 40 )) h5_path = r \"\\product0\\Bxyz.h5\" r . write_hdf5 ( array_data , h5_path , overwrite = True )","title":"Write array data"}]}